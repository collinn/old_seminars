---
title: "Tips and Tricks"
author: "PDAC Meeting"
date: "11/19/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Methods

Methods and generics in R allow us to express a generic idea (such as plot, print, summary) and dispatch an associated function, relative to it's class. For example

```{r}
x <- matrix(rnorm(200), ncol = 2)
y <- lm(Sepal.Length ~ Sepal.Width + Petal.Length, data = iris)

plot(x)
plot(y, which = 1)
```

We can see the methods availabe to us with the `methods` call

```{r}
class(x); class(y)
plot
methods(plot)
?plot.default
?plot.lm
```

This is implemented in a pretty straightforward manner, and are written of the form `generic.class()` (which also emphasizes why we should use underscores instead of periods for function names)

```{r}
## The generic, only calls to method
donothing <- function(x) {
  UseMethod("donothing")
}

## In case a particular method isn't found, use default
donothing.default <- function(x) {
  print("this is default, you should usually include this")
}

## Class specific methods
donothing.matrix <- function(x) {
  print("this is a matrix")
}

donothing.data.frame <- function(x) {
  print("maybe consider using data.table instead")
}

donothing.newclass <- function(x) {
  print("helpful if you want to extend previous methods with your own class")
}

z <- list(1:10)
donothing(z)

x <- matrix(rnorm(10))
donothing(x)

y <- as.data.frame(x)
donothing(y)

## Say w 'inherits' from y
w <- structure(y, class = c("newclass", class(y)))
donothing(w)
```

## .Rprofile and startup

There may be a few things that you wish to do each time you work with R. One of the options available to us is a configuration file known as an [.Rprofile](https://www.r-bloggers.com/fun-with-rprofile-and-customizing-r-startup/). A user can have multiple .Rprofile files, often there is a system wide file, one in a user's home directory, as well as one in a project directory (which takes precedence if R is loaded from said directory). Generally none of these are generated by default, and if they are, it's usually the system wide version. Some things to keep in mind

- Ok to load packages automatically if using interactively
- Helpful to source functions used in analysis
- Ok to set some system options, but BE CAREFUL
- Creating a startup environment allows you to load functions and not worry if you clear the environment with (rm(list = ls()))

We will take a look at an example .Rprofile in RStudio, but for those coming back to this document, a few nice things to include (many taken from linked R-Bloggers page)

```{r}
## Source all R files in a directory
sourceDir <- function(path, trace = FALSE, ...) {
  for(nm in list.files(path, pattern = "\\.[Rr]")) {
    if(trace) cat(nm, ":")
    source(file.path(path, nm), ...)
    if(trace) cat("\n")
  }
}

## This is bad. Why?
options(stringsAsFactors=FALSE) 

## This is OK, scientific notation is hard
options(scipen=50)

## Format printing output
options(width=80, digits = 5)

## Especially nice when quitting from command line
q <- function (save="no", ...) {
  quit(save=save, ...)
}
 
## Not sure why this isn't default, allows autocomplete of library names
utils::rc.settings(ipck=TRUE)

## I don't typically use these, but they are an excellent idea
## This timestamps (with directory) all of the interactive R code you run
.First <- function(){
  if(interactive()){
    library(utils)
    timestamp(,prefix=paste("##------ [",getwd(),"] ",sep=""))
  }
}

## Then when exiting, it saves your (timestamped) interactive
## history into either a system designated file (R_HISTFILE) or
## into a hidden .Rhistory file in your home directory
.Last <- function(){
  if(interactive()){
    hist_file <- Sys.getenv("R_HISTFILE")
    if(hist_file=="") hist_file <- "~/.RHistory"
    savehistory(hist_file)
  }
}

# ## Make new environment for startup functions (won't be erased with (rm()))
# .startup <- new.env()
# .startup$somefunction <- function() do something
# attach(.startup)
# sys.source("startfuns.R", envir = attach(NULL, name = ".startup"))
```

## Be Careful!

There are a few places that can unsuspectingly cause difficulty. This comes from the patchwork nature of R, and as such, it's good to be aware of some of the places that things can go wrong. First and absolutely foremost (unrelated to being careful, but absolutely essential practice), avoid [Magic Numbers](https://en.wikipedia.org/wiki/Magic_number_(programming)) at all cost. This means you should never write something like

```{r}
x <- 1:10

## No
for(i in 1:10) {
  x[i] <- x[i]^2
}
```

What if you have this written 10,000 times in your code and the length of `x` changes? Far better to be explicit in what you want, i.e., 

```{r}
x <- 1:10

## Better, I guess
for(i in 1:length(x)) {
  x[i] <- x[i]^2
}
```

Even better, when you are working with vector operations, skip the loop altogether. This holds for basically all pointwise vector operations. R is clever enough to know what you want when you use vectorized arithmetic in the presence of scalars as well (using recycling, as we will see below)

```{r}
x <- rnorm(10)
y <- rnorm(10)
z <- 1

x*y + z*y - exp(x) + x/y

# ## !!!
# x*x != t(x) %*% x
```

Now, despite what was said above for illustrative purposes, we can do one better to anticipate unanticipated changes in our code. First, let's take a moment to familiarize ourselves with a length zero numeric vector. It is a numeric vector of zero length (duh), but this all means that operations on it are maybe not what you expect

```{r}
x <- numeric(0L)
x
x + 10

identical(x, 0L)
```

This can be a particularly sneaky error in our loops

```{r}
x <- numeric(0L)

## Incorrect
for(i in 1:length(x)) {
  print(i)
}
```

Instead, the function `seq_along` gives us the expected output

```{r}
## Correct
for(i in seq_along(x)) {
  print(i)
}
```

Ok, one more place where I lied about what to do. We saw above that `x <- numeric(0L)` returns a vector of zero length. Does this generalize to other positive integers?

```{r}
## Neat!
x <- numeric(5)
x
length(x)

## Less neat
x <- list(5)
x
length(x)
```

We can remedy this again by being more explicit about what we want

```{r}
x <- vector("numeric", length = 5L)
y <- vector("list", length = 5L)
x
y
```

We do still see a difference here, unfortunately, and that is in what R chooses to initialize each of the vectors with. I think it is good practice to initialize values with something like `NA` to differentiate between a true zero and an element that was never assigned a value. For example, suppose we want to perform a collection of computations that have a non-zero possibility of failing, and in which `0` is also a plausible solution

```{r}
## Initialized with 0
y <- vector("numeric", length = 1000L)

set.seed(69)
for(i in seq_along(y)) {
  u <- runif(1)
  if(u < 0.8) {
    y[i] <- rbinom(1, 1, 0.75)
  }
}

table(y, useNA = "always")

## Initialized with NA
y <- NA*vector("numeric", length = 1000L)

set.seed(69)
for(i in seq_along(y)) {
  u <- runif(1)
  if(u < 0.8) {
    y[i] <- rbinom(1, 1, 0.75)
  }
}

table(y, useNA = "always")
```


## Don't be unnecessarily slow

One of the simplest fixes to slow code is to look for places where we are making copies in memory. This happens any time we use `c()`, `rbind()`, or `cbind()`, for example. Better to initialize a vector (or list) of the appropriate size, and then fill in as necessary. This allows R to preallocate the memory it will need rather than making copies as the need arises

```{r}
library(rbenchmark)

grow_vec <- function(n) {
  x <- c()
  for(i in seq_len(n)) {
    x <- cbind(x, rnorm(1))
  }
}

init_vec <- function(n) {
  x <- vector("numeric", n)
  for(i in seq_len(n)) {
    x[i] <- rnorm(1)
  }
}


benchmark(
  grow_vec(n = 10000),
  init_vec(n = 10000),
  replications = 25
)
```


We've seen a few examples so far, and most all of them have used dreaded `for` loops, carrying with them accusations of inefficiency and being slow. However, let's consider the following ways in which we may complicate finding the column means of a matrix

```{r, cache=TRUE}
set.seed(69)
x <- matrix(rnorm(100000*100), ncol = 100)

## Using apply
f_apply <- function(x) {
  res <- apply(x, 2, mean)
}

## Loops
f_loop <- function(x) {
  res <- vector("numeric", ncol(x))
  for(i in seq_along(ncol(x))) {
    res[i] <- mean(x[, i])
  }
}

f_loop2 <- function(x) {
  res <- vector("numeric", ncol(x))
  n <- nrow(x)
  for(i in seq_along(ncol(x))) {
    res[i] <- sum(x[, i])/n
  }
}

## And lapply
f_lapply <- function(x) {
  res <- lapply(as.data.frame(x), mean, numeric(1))
  unlist(res, use.names = FALSE)
}

## And colMeans
benchmark(f_apply(x), 
          f_loop(x),
          f_loop2(x),
          f_lapply(x),
          colMeans(x),
          replications = 50)

```

What's perhaps most surprising here is the roughly 18 times speed increase that our `for` loop has over the built-in `colMeans` function, much less the 300 times increase associated with `apply`. Why might this be? We can get some insight if we take a peek at the source code for each, where we see that the R implementation of these functions carries with them a lot of additional overhead outside of simplying appying column means. This is further evidenced by the fact that the loop utilizing `sum/length` is  slightly faster than simply calling the `mean` funciton (though `mean` will be more numerically accurate in any instance in which the values are not identical). And if we consider `apply`, we can see that it's actually an even nastier, more complicated loop that's just been hidden from us

```{r}
apply
```

`lapply` is only slightly better here, though it's still a `for` loop, just written in C. We can compare even further by considering the `vapply` function, which is similar to `lapply`, but with the additional constraint that one must prespecify the output. Naturally, we might suspect that having prespecified the output would lead to a substantial performance gain:

```{r, cache = TRUE}
x <- rpois(n = 500000, lambda = 20)

benchmark(
  lapply(x, function(y) sqrt(y) < 5),
  vapply(x, function(y) (sqrt(y) < 5), logical(1)),
  replications = 25
)
```

Substantial indeed. However, what we are failing to account for here is two things

- `vapply` is going to return a numeric vector, rather than a list (which needs to be unlisted)
- `vapply` gives us type-checking, throwing an error when our function fails to return the appropriate output

Each of these are non-trivial benefits of `vapply`, and we verify the saved overhead by giving a more appropriate comparison

```{r, cache = TRUE}

## Unlist AND do type-checking
f_lapply2 <- function(x) {
  res <- lapply(x, function(y) {
    tt <- (sqrt(y) > 5)
    ## Type checking
    if(!is.logical(tt) | length(tt) != 1) stop("stop")
  })
  ## Unlisting
  unlist(res, use.names = FALSE)
}

benchmark(
  vapply(x, function(y) sqrt(y) > 5, logical(1)),
  f_lapply2(x),
  replications = 25
)
```


We see that functionals do still have their place, both by performing safety checks and formatting output, and by making code more expressive and easier to read. The main take away here is that the performance cost is minor in day to day work, but if you do make use of the `apply` functionals, especially in computationally heavy simulation, it may be worth investigating what can be saved with an explicit `for` loop (with appropriate type checking, of course).

<!-- Now, one could argue that sometimes R gives us a bit more than we actually need. A lot of functions in base R are exceptionally powerful, yet the power comes with a (demonstrated) cost. By making our code *less* general, we can sometimes get a markup in performance. The following example comes from *Advanced R*, where given a numeric vector `x`, we restrict all values to lie within the interval `[a, b]` -->

<!-- ```{r, cache = TRUE} -->
<!-- ## Squish numeric vector x into interval [a, b] -->
<!-- squish_ife <- function(x, a, b) { -->
<!--   ifelse(x <= a, a, ifelse(x >= b, b, x)) -->
<!-- } -->

<!-- squish_p <- function(x, a, b) { -->
<!--   pmax(pmin(x, b), a) -->
<!-- } -->

<!-- squish_in_place <- function(x, a, b) { -->
<!--   x[x <= a] <- a -->
<!--   x[x >= b] <- b -->
<!--   x -->
<!-- } -->

<!-- x <- rpois(n = 50000, lambda = 20) -->
<!-- benchmark(squish_ife(x, 15, 25), -->
<!--           squish_p(x, 15, 25), -->
<!--           squish_in_place(x, 15, 25)) -->
<!-- ``` -->


Hinted at above, if we *do* run `lapply` (or any other function that returns a list), and we wish to unlist it, we can see significant performance gains by passing an additional argument to `unlist`

```{r, cache = TRUE}
x <- matrix(rnorm(1e6*2), nrow = 2)
res <- lapply(as.data.frame(x), mean)

## We often don't need the names of the listed elements
benchmark(
unlist(res),
unlist(res, use.names = FALSE)
)
```

One common use of `lapply` is the generation of simulated observations, reading in files, or some other set of operations resulting in a `data.frame`. The `rbindlist` function, from the `data.table` package, gives us an excellent way to speed up the process of generating a single `data.frame` from the list of `data.frames` we start with. As an added bonus, you end up with a `data.table` instead

```{r, cache = TRUE}
library(data.table)

datasets <- lapply(1:1000, function(x) {
  dat <- data.frame(ID = rep(x, 15), 
                    A = rnorm(15),
                    B = rnorm(15),
                    C = rnorm(15))
})

f_loop <- function(x) {
  res <- data.frame()
  for(i in seq_along(x)) {
    res <- rbind(res, x[[i]])
  }
}

benchmark(
  f_loop(datasets),
  Reduce(rbind, datasets),
  rbindlist(datasets),
  replications = 10
)

# ## My typical pattern
# result <- lapply(datasets, function(x) {
#   # do something
# }) %>% rbindlist()
```

## Useful Functions
Here, we introduce a couple of helpful wrapper functions. While these may or may not be useful to you in particular, they do illustrate ways to combine functions in R to create new tools that may be of use to you. One of the simplest I use is built around `object.size`

```{r}
x <- matrix(rnorm(1e6*2), nrow = 2)
object.size(x)
?object.size

size_of <- function(x, units = "Mb") {
  format(object.size(x), units = units)
}

size_of(x)
```

The next two are less trivial. First, we consider a function that filters out null values from a list

```{r}
## Function to remove null values from list
compact <- function(x) {
  Filter(Negate(is.null), x)
}

x <- vector("list", 5)
for(i in c(1, 3, 5)) {
  x[[i]] <- i
}

x
(x <- compact(x))
```

Now suppose first that we are running an analysis in which we fit models on collections of simulated data, some of which are destined to fail

```{r}
## Simulate model failure
dummy_fit <- function() {
  u <- runif(1)
  if(u < 0.8) {
    res <- lm(Sepal.Width ~ Sepal.Length + Petal.Length, data = iris)
  } else {
    res <- NULL
  }
  return(res)
}

set.seed(69)
model_fits <- replicate(20, dummy_fit())
```

We can then use our compact function to remove those that failed while retaining ones that are ready for further work

```{r}
length(model_fits)
length(compact(model_fits))
```

Finally, we consider the `where` function, a painfully absent function in R, though with a  close cousin, `which`. `which`, given a vector of `TRUE/FALSE` values, returns indicies of those values which are `TRUE`. `where`, on the other hand, is given a vector and a logical expression, and returns a logical vector. These names probably *ought* to be flipped, however, `which` is in base R and was named first, so here we are. Note one benefit below is that `which` can take an expression (returning a vector of logicals) whereas `where` must take a function as an argument

```{r}
## First which
x <- 1:10
idx <- which(x < 5)
x[idx]


## Then where
where <- function(x, f, only.true = FALSE) {
  res <- vapply(x, f, logical(1))
  if(only.true) return(names(res[res == TRUE]))
  res
}

## Subsetting by logicals equivalent to subsetting by position
idx <- where(x, function(y) (y < 5))
x[idx]
x[which(idx)]


## Where is the iris dataset numeric?
(vars <- where(iris, is.numeric))

## Maybe we only want where it's true
(num_vars <- where(iris, is.numeric, only.true = TRUE))
```

Perhaps the default option of `only.true` makes more sense to be set to `TRUE`. Or maybe you think `only.true` is a stupid name for an argument and you want it to change it to something else. Good news! You can, it's your function now, do with it what you will. 

One benefit to keeping only true names is to perform class specific operations

```{r}
## Only vars that are numeric
(num_vars <- where(iris, is.numeric, only.true = TRUE))
mean_val <- vector("numeric", length(num_vars))
names(mean_val) <- num_vars

for(var in num_vars) {
  mean_val[var] <- mean(iris[[var]])
}
```

## Bonus!

Here are some bonus tips that show off a bit more of what to expect with the vectorization, as implemented in R

```{r}
library(data.table)

## Vector recycle
x <- 1:20
x

## Recycle 2, get evens
x[c(FALSE, TRUE)]

## Recycle 3, get multiples of 3
x[c(FALSE, FALSE, TRUE)]

## Make column names
(col_names <- paste0("col", seq_len(10)))


## Vectorization
x <- seq_len(10)
y <- NA*x
z <- NA*x

## Not vectorized :(
if(x < 5) {
  y <- x
} else {
  y <- 5
}
print(y)

## Vectorized!
ifelse(x < 5, z <- x, z <- 5)

x <- c(TRUE, TRUE, FALSE)
y <- c(TRUE, FALSE, FALSE)

## Only inspects first element
x && y
x || y

## Inspects all elements
x & y
x | y

## Sure would be nice if this worked...
# USArrests["Murder" > 13.2, ]

usa <- as.data.table(USArrests, keep.rownames = TRUE)
head(usa)

## Here we are subsetting by logical vectors
usa[Murder > 13.1 & Assault > 230, ]

## And again, but with a different logical vector
usa[Murder > 13.1 && Assault > 230, ]


## What do those vectors look like?
attach(usa) # <- adds var names of usa to global environment

## Returns appropriate length
Murder > 13.1 & Assault > 230

## Recycles length one vector, selecting all columns
Murder > 13.1 && Assault > 230
```

## Sources
Basically everything from this document came from the ideas or work of others. In particular, excellent references are [Advanced R](https://adv-r.hadley.nz/) by Hadley Wickham and [R Inferno](https://www.burns-stat.com/documents/books/the-r-inferno/) by Patrick Burns. Other sources include [R documentation](https://cran.r-project.org/doc/manuals/r-release/R-lang.html), the [R Internals Manual](https://cran.r-project.org/doc/manuals/r-release/R-ints.html), and [Stack Exchange](https://stackexchange.com/).















